# Requirements

The key thing of the tests is that we want to run an "normal" workload. That is
to say, we want to run a Linux kernel target initiator against it. For this we
use VMs. Using a VM is not required but it may cause dangling devices in your
kernel that might be a hard to get rid of.

Test may define their own `docker-compose.yml` to ensure it has the right
environment. Its possible however, that you are not able to run all the tests
on your local machine. You can change the `docker-compose.yml` to match your
needs. For example, to lower the amount of cores.

# Configuring the test environment

For configuration of a run or different runs, we make use of `pytest-testconfig`. This
file contains settings available in each run. This file can be a python script
or a plain configuration file.

Currently, it contains a single variable. It is advised to tailor it to your
needs and use it as an argument. i.e:

```
pytest $test --tc-file=TESTCONFIG
```

or use an environment variable, for example:

```
export PY_TEST_CONFIG_FILE=/path/to/my/config.ini
```

# Converting a .feature

A feature can automatically be converted to python code. This is not required
but avoids mismatch between the two. An advantage of using these features is
that others do not have to figure out what the test is supposed to be testing.

```
pytest-bdd generate xxx.feature > test_xxx.py

```

When new scenarios are added the files can be updated with:

```
pytest --generate-missing --feature pool_create.feature test_pool.py

```

# Setup virtual env

Not all packages are available on nix, so one extra step is needed if you run
nix.

```shell
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf` --python_out=test/python --grpc_python_out=test/python mayastor.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf` --python_out=test/python --grpc_python_out=test/python csi.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python registration.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python host.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python common.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python bdev.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python replica.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python pool.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python nexus.proto
python -m grpc_tools.protoc --proto_path=`realpath rpc/mayastor-api/protobuf/v1` --python_out=test/python --grpc_python_out=test/python snapshot.proto

virtualenv --no-setuptools test/python/venv
source test/python/venv/bin/activate
pip install -r test/python/requirements.txt
```

The virtual environment must be activated for every shell. Consider using something as `direnv` to automate this.
The `.proto` files, generated by the tool should never be committed and are already part of `.gitignore`.

# Running the tests

Running an individual test:
`python -m pytest --tc-file=test_config.ini --docker-compose=tests/replica tests/replica/test_bdd_pool.py`

Running all tests within a directory:
`python -m pytest --tc-file=test_config.ini --docker-compose=tests/replica tests/replica`

# Running tests with existing containers

If you need to debug or want the environment not cleaned up you can start the containers
manually. For example:

```
docker-compose up
# different termimal
python -m pytest --tc-file=$path_to_my_config $test_name --use-running-containers --docker-compose-no-build -s
```
